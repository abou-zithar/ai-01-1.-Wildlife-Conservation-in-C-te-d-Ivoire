{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"sea_creatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(\"sea_creatures\",\"train\")\n",
    "classes = os.listdir(train_dir)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 224\n",
    "width = 224\n",
    "\n",
    "\n",
    "class ConvertToRGB:\n",
    "    def __call__(self, img):\n",
    "        if img.mode != \"RGB\":\n",
    "            img = img.convert(\"RGB\")\n",
    "        return img\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    ConvertToRGB(),                       # Ensure the image is in RGB format\n",
    "    transforms.Resize((width, height)),   # Resize to 224x224\n",
    "    transforms.PILToTensor(),             # Convert to a PyTorch tensor\n",
    "    transforms.ConvertImageDtype(torch.float)  # Optional: normalize to float\n",
    "])\n",
    "\n",
    "print(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file = \"sea_creatures/train/Dolphin/10004986625_0f786ab86b_b.jpg\"\n",
    "\n",
    "image = Image.open(sample_file) # load your image\n",
    "\n",
    "transformed_image = transform(image)\n",
    "print(transformed_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset =datasets.ImageFolder(\n",
    "    root=train_dir,  # Replace with the path to your training data folder\n",
    "    transform=transform   # Apply the transformer pipeline\n",
    ")\n",
    "print(\"Image size\", dataset[0][0].shape)\n",
    "print(\"Label\", dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will get us the counts, but notice that the keys are the class indices,\n",
    "# not the class names.\n",
    "counts = Counter(x[1] for x in tqdm(dataset))\n",
    "print(\"The counts dictionary:\", counts)\n",
    "\n",
    "# This dictionary maps class names to their index.\n",
    "print(\"The class_to_idx dictionary:\", dataset.class_to_idx)\n",
    "\n",
    "# Use both of these to construct the desired dictionary\n",
    "\n",
    "class_distribution = {Class:counts[dataset.class_to_idx[Class]] for Class in classes}\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,        # Adjust batch size as needed\n",
    ")\n",
    "\n",
    "# Get one batch\n",
    "first_batch = next(iter(dataset_loader))\n",
    "\n",
    "print(f\"Shape of one batch: {first_batch[0].shape}\")\n",
    "print(f\"Shape of labels: {first_batch[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(loader):\n",
    "    \"\"\"Computes the mean and standard deviation of image data.\n",
    "\n",
    "    Input: a `DataLoader` producing tensors of shape [batch_size, channels, pixels_x, pixels_y]\n",
    "    Output: the mean of each channel as a tensor, the standard deviation of each channel as a tensor\n",
    "            formatted as a tuple (means[channels], std[channels])\"\"\"\n",
    "\n",
    "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
    "    for data, _ in tqdm(loader):\n",
    "        channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
    "        channels_squared_sum += torch.mean(data**2, dim=[0, 2, 3])\n",
    "        num_batches += 1\n",
    "    # Compute the mean from the channels_sum and num_batches\n",
    "    mean = channels_sum / num_batches\n",
    "    # Compute the standard deviation form channels_squared_sum, num_batches,\n",
    "    # and the mean.\n",
    "    std = torch.sqrt((channels_squared_sum / num_batches) - (mean ** 2))\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "mean, std = get_mean_std(dataset_loader)\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Standard deviation: {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_norm = transforms.Compose([\n",
    "transforms.Resize((224, 224)), # Resize image to 224x224\n",
    "transforms.ToTensor(), # Convert image to a tensor (C, H, W)\n",
    "transforms.Normalize(mean=mean, std=std) # Normalize the image channels\n",
    "])\n",
    "\n",
    "print(transform_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dataset =datasets.ImageFolder(\n",
    "    root=train_dir,  # Replace with the path to your training data folder\n",
    "    transform=transform_norm   # Apply the transformer pipeline\n",
    ")\n",
    "# print(\"Image size\", norm_dataset)\n",
    "\n",
    "print(\"Image size\", norm_dataset[0][0].shape)\n",
    "print(\"Label\", norm_dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for reproducibility\n",
    "g = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset = random_split(dataset, [.8, .2], generator=g)\n",
    "\n",
    "print(\"Training data set size:\", len(train_dataset))\n",
    "print(\"Validation data set size:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),  # Conv2D layer\n",
    "    nn.ReLU(),  # ReLU activation\n",
    "    nn.MaxPool2d(kernel_size=4, stride=4),  # Max pooling\n",
    ")\n",
    "\n",
    "# ... your layers here ...\n",
    "summary(model, input_size=(batch_size, 3, height, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),  # Conv2D layer\n",
    "    nn.ReLU(),  # ReLU activation\n",
    "    nn.MaxPool2d(kernel_size=4, stride=4),  # Max pooling\n",
    "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),  # Conv2D layer\n",
    "    nn.ReLU(),  # ReLU activation\n",
    "    nn.MaxPool2d(kernel_size=4, stride=4),  # Max pooling\n",
    ")\n",
    "\n",
    "# Add these layers to the model\n",
    "\n",
    "summary(model, input_size=(batch_size, 3, height, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),  # Conv2D layer\n",
    "    nn.ReLU(),  # ReLU activation\n",
    "    nn.MaxPool2d(kernel_size=4, stride=4),  # Max pooling\n",
    "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),  # Conv2D layer\n",
    "    nn.ReLU(),  # ReLU activation\n",
    "    nn.MaxPool2d(kernel_size=4, stride=4),  # Max pooling\n",
    "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),  # Conv2D layer\n",
    "    nn.ReLU(),  # ReLU activation\n",
    "    nn.MaxPool2d(kernel_size=4, stride=4),  # Max pooling\n",
    "    nn.Flatten() \n",
    ")\n",
    "# Add the new layers\n",
    "\n",
    "summary(model, input_size=(batch_size, 3, height, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),  \n",
    "    nn.ReLU(),  \n",
    "    nn.MaxPool2d(kernel_size=4, stride=4),  \n",
    "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1), \n",
    "    nn.ReLU(),  \n",
    "    nn.MaxPool2d(kernel_size=4, stride=4),  \n",
    "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),  \n",
    "    nn.ReLU(),  \n",
    "    nn.MaxPool2d(kernel_size=4, stride=4), \n",
    "    nn.Flatten(),  \n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(576, 500),  # Assuming input images were 32x32 (after pooling size becomes 2x2)\n",
    "    nn.ReLU(),  \n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(500, 9))\n",
    "# Add the final layers\n",
    "\n",
    "summary(model, input_size=(batch_size, 3, height, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # lr can be adjusted\n",
    "\n",
    "# Send the model to the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the train and predict functions from `training.py`, instead of typing them out!\n",
    "from training import train, predict  # Import train and predict functions from training.py\n",
    "\n",
    "epochs = 10\n",
    "# Train the model for 10 epochs\n",
    "\n",
    "train(model, optimizer, loss_fn, train_loader, val_loader, epochs=10, device=device)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the probabilities for each validation image\n",
    "probabilities = predict(model, val_loader, device=device)\n",
    "\n",
    "# Get the index associated with the largest probability for each\n",
    "predictions = torch.argmax(probabilities,axis=1)\n",
    "\n",
    "print(\"Number of predictions:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "\n",
    "for _, labels in tqdm(val_loader):\n",
    "    targets.extend(labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor(targets).shape\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Don't change this\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "cm = confusion_matrix(torch.Tensor(targets).to('cpu'), predictions.to('cpu'))\n",
    "\n",
    "# Get the class names\n",
    "classes = classes\n",
    "\n",
    "# Display the confusion matrix (don't change this)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "disp.plot(cmap=plt.cm.Blues, xticks_rotation=\"vertical\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = os.path.join(\"sea_creatures\",\"test\")\n",
    "\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "print(\"Number of test images:\", len(test_dataset))\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the probabilities for each test image\n",
    "test_probabilities = predict(model, test_loader, device=device)\n",
    "\n",
    "# Get the index associated with the largest probability for each test image\n",
    "test_predictions = torch.argmax(probabilities,axis=1)\n",
    "\n",
    "print(\"Number of predictions:\", test_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classes = [classes[i] for i in test_predictions]\n",
    "\n",
    "print(\"Number of class predictions:\", len(test_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Sample 12 random indices from the test dataset\n",
    "sample_indices = random.sample(range(len(test_loader.dataset.samples)), 12)\n",
    "\n",
    "# Create a grid of 4x3 subplots\n",
    "fig, axes = plt.subplots(4, 3, figsize=(20, 10))\n",
    "\n",
    "# Iterate over the sampled indices and plot the corresponding images\n",
    "for ax, idx in zip(axes.flatten(), sample_indices):\n",
    "    image_path = test_loader.dataset.samples[idx][0]\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Display the image on the axis\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Get the predicted class for this image\n",
    "    predicted_class = test_classes[idx]\n",
    "\n",
    "    # Set the title of the subplot to the predicted class\n",
    "    ax.set_title(f\"Predicted: {predicted_class}\", fontsize=14)\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
